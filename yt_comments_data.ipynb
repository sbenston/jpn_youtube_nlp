{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8880a420",
   "metadata": {},
   "source": [
    "# YouTube Comments via YouTube API\n",
    "\n",
    "This notebook utilizes Google's YouTube Data API to generate a dataset of YouTube comments from Japanese language videos that are trending in the region. It takes up to 250 videos depending on how many videos in the most popular chart for the region are categorized as being in the language to increase chances that comments on the video are in Japanese. From each video, up to 100 comments are grabbed along with all their replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from pathlib import Path\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in credentials from environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "# Initialize API client\n",
    "youtube = build(\n",
    "    'youtube', 'v3', developerKey=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35817e44",
   "metadata": {},
   "source": [
    "## Retrieve comments from YouTube\n",
    "\n",
    "commentThreads() with 'snippet,replies' pulls comments and all their replies from a specified videoId.\n",
    "- order='relevance' gets comments with lots of likes, which are more likely to have replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comments(video_id, max_results=10):\n",
    "    \n",
    "    # Make request to API and save as a variable\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet,replies',\n",
    "        maxResults=max_results,\n",
    "        order='relevance',\n",
    "        videoId=video_id\n",
    "    )\n",
    "    try:\n",
    "        response = request.execute()\n",
    "        \n",
    "        return response\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "# test_response = retrieve_comments(video_id='4V0UAhe8o5c')\n",
    "# test_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c07c1",
   "metadata": {},
   "source": [
    "## Writing comment data to a Pandas DF\n",
    "\n",
    "Takes in a response from an API call and a dictionary containing information about the video. Returns a dataframe created from a list of dictionaries that contain:\n",
    "\n",
    "* channel - the channel's title\n",
    "* video_id - YouTube's unique identifier of the video\n",
    "* category_id - YouTube id that classifies what the content of the video is about\n",
    "* text - original text of the comment before any edits\n",
    "* date_published - date when the comment was made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68390be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(response, video):\n",
    "    \n",
    "    comment_data = []\n",
    "    \n",
    "    for item in response['items']:\n",
    "        \n",
    "        # Grab the top-level comment first\n",
    "        comment_data.append({\n",
    "            'channel': video['channel'],\n",
    "            'video_id': video['video_id'],\n",
    "            'category_id': video['category_id'],\n",
    "            'text': item['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "            'date_published': item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "        })\n",
    "        \n",
    "        # Check if there are replies and get same info if there are\n",
    "        if 'replies' in item.keys():\n",
    "\n",
    "            for reply in item['replies']['comments']:\n",
    "\n",
    "                comment_data.append({\n",
    "                    'channel': video['channel'],\n",
    "                    'video_id': video['video_id'],\n",
    "                    'category_id': video['category_id'],\n",
    "                    'text': reply['snippet']['textOriginal'],\n",
    "                    'date_published': reply['snippet']['publishedAt']\n",
    "                })\n",
    "        \n",
    "    return pd.DataFrame(comment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bec67",
   "metadata": {},
   "source": [
    "## Retrieve popular videos\n",
    "\n",
    "videos().list() method with chart='mostPopular' generates a resource that has information about the currrently trending videos within a region.\n",
    "\n",
    "There is a limit on the amount of videos displayed in the response but within the results is a token is given that can generate the next page of results. The function accepts this token as a parameter, which when combined with a loop allows for all the results to be retrieved up to the maximum specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_videos(next_page=''):\n",
    "    request = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        chart='mostPopular',\n",
    "        maxResults=250,\n",
    "        pageToken=next_page,\n",
    "        regionCode='JP',\n",
    "    )\n",
    "    response = request.execute()\n",
    " \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_videos(response):\n",
    "    \n",
    "    for item in response['items']:\n",
    "        \n",
    "        snippet = item['snippet']\n",
    "        \n",
    "        if (snippet.get('defaultLanguage') == 'ja' or\n",
    "            snippet.get('defaultAudioLanguage') == 'ja'):\n",
    "      \n",
    "            videos.append({\n",
    "                'video_id': item['id'],\n",
    "                'channel': snippet['channelTitle'],\n",
    "                'category_id': snippet['categoryId']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list\n",
    "videos = []\n",
    "\n",
    "# Run once to generate first page\n",
    "response = retrieve_videos('')\n",
    "next_page = response['nextPageToken']\n",
    "list_videos(response)\n",
    "\n",
    "# Loop until there is no nextPageToken\n",
    "try:\n",
    "    while next_page:\n",
    "        response = retrieve_videos(next_page)\n",
    "        next_page = response['nextPageToken']\n",
    "        list_videos(response)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check list of videos\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store data\n",
    "comments_df = pd.DataFrame(columns=['channel',\n",
    "                                    'video_id',\n",
    "                                    'category_id',\n",
    "                                    'text',\n",
    "                                    'date_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through videos list       \n",
    "for video in videos:\n",
    "    \n",
    "    # Query the YouTube Data API\n",
    "    response = retrieve_comments(video['video_id'], max_results=100)\n",
    "    sleep(3)\n",
    "\n",
    "    # Add the data from the response to the DF\n",
    "    if response:\n",
    "        data = extract_info(response, video)\n",
    "        comments_df = pd.concat([comments_df, data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check DF\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c174da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the comment data to a CSV file\n",
    "output_path = Path('Resources/youtube_comments.csv')\n",
    "comments_df.to_csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
